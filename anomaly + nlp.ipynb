{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b40a7dba-ce31-41de-a9d8-646401d3673d",
   "metadata": {},
   "source": [
    "# Downloading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd05e219-1919-4384-ac88-32f5b345051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpha_vantage.timeseries import TimeSeries\n",
    "import pandas as pd\n",
    "\n",
    "# Replace with your Alpha Vantage API key\n",
    "API_KEY = \"your api key\"\n",
    "\n",
    "# Initialize Alpha Vantage TimeSeries API\n",
    "ts = TimeSeries(key=API_KEY, output_format=\"pandas\")\n",
    "\n",
    "# Fetch Adani Ports data (BSE)\n",
    "stock_data, meta_data = ts.get_daily(symbol=\"BHEL.BSE\", outputsize=\"full\")\n",
    "\n",
    "# Rename columns for readability\n",
    "stock_data = stock_data.rename(columns={\n",
    "    \"1. open\": \"Open\",\n",
    "    \"2. high\": \"High\",\n",
    "    \"3. low\": \"Low\",\n",
    "    \"4. close\": \"Close\",\n",
    "    \"5. volume\": \"Volume\"\n",
    "})\n",
    "\n",
    "# Convert index to datetime\n",
    "stock_data.index = pd.to_datetime(stock_data.index)\n",
    "\n",
    "# Display the first few rows\n",
    "print(stock_data.head())\n",
    "\n",
    "# Save to CSV for later use\n",
    "stock_data.to_csv(\"bhel_stock_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bae4774-872a-49be-ade2-bf1e9af1fa2d",
   "metadata": {},
   "source": [
    "# Fetching news for different indian companies and performing sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16425d3e-1fa7-4adf-ac92-32a3c69477c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download once\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_top_news_sentiment(company_name):\n",
    "    query_url = company_name.replace(\" \", \"+\")\n",
    "    url = f\"https://news.google.com/rss/search?q={query_url}+when:1d&hl=en-IN&gl=IN&ceid=IN:en\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, features=\"xml\")\n",
    "    item = soup.find(\"item\")\n",
    "\n",
    "    if item is None:\n",
    "        print(f\"❌ No news found for {company_name}.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Extract details\n",
    "    title = item.title.text\n",
    "    link = item.link.text\n",
    "    pub_date = item.pubDate.text\n",
    "    pub_date_fmt = datetime.strptime(pub_date, \"%a, %d %b %Y %H:%M:%S %Z\").strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Sentiment analysis\n",
    "    score = sia.polarity_scores(title)['compound']\n",
    "    sentiment = (\n",
    "        \"Positive\" if score >= 0.05 else\n",
    "        \"Negative\" if score <= -0.05 else\n",
    "        \"Neutral\"\n",
    "    )\n",
    "\n",
    "    # Save to DataFrame\n",
    "    df = pd.DataFrame([{\n",
    "        # \"company\": company_name,\n",
    "        \"headline\": title,\n",
    "        # \"link\": link,\n",
    "        \"Date\": pub_date_fmt,\n",
    "        \"sentiment_score\": score,\n",
    "        \"sentiment_label\": sentiment\n",
    "    }])\n",
    "\n",
    "    filename = f\"{company_name.lower().replace(' ', '_')}_news_sentiment.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"✅ Top news for {company_name} saved to: {filename}\")\n",
    "    return df\n",
    "\n",
    "# Example: Get news for ITC Limited or any other company\n",
    "company = \"bhel\"\n",
    "top_news_df = get_top_news_sentiment(company)\n",
    "print(top_news_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868ac900-f449-46dd-bc0e-50fae8ea662d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Merging the stock dataset and news sentiment dataset of each company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b96448-dabc-4320-bfbb-29de6292e634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load stock data (up to March 18)\n",
    "df_stock = pd.read_csv(\"bhel_stock_data.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "# Load news sentiment data (from March 19)\n",
    "df_news = pd.read_csv(\"bhel_news_sentiment.csv\", parse_dates=[\"Date\"])\n",
    "\n",
    "# **Shift today's news (March 19) backward by 1 day (so it merges with March 18 stock prices)**\n",
    "df_news[\"Date\"] = df_news[\"Date\"] - pd.Timedelta(days=1)\n",
    "# print(df_news[\"Date\"])\n",
    "\n",
    "# Convert date format to ensure proper merging\n",
    "df_stock[\"date\"] = df_stock[\"date\"].dt.date\n",
    "df_news[\"Date\"] = df_news[\"Date\"].dt.date\n",
    "\n",
    "# **Merge March 19 News with March 18 Stock Data**\n",
    "df_merged = pd.merge(df_stock, df_news, left_on=\"date\", right_on=\"Date\", how=\"outer\")\n",
    "\n",
    "# Fill missing news sentiment with forward-fill (if past news is available)\n",
    "df_merged[\"headline\"] = df_merged[\"headline\"].fillna(method=\"ffill\")\n",
    "df_merged[\"sentiment_score\"] = df_merged[\"sentiment_score\"].fillna(method=\"ffill\")\n",
    "\n",
    "# Fill any remaining NaN values with 0\n",
    "df_merged[\"sentiment_score\"] = df_merged[\"sentiment_score\"].fillna(0)\n",
    "\n",
    "# Drop duplicate Date column\n",
    "df_merged.drop(columns=[\"Date\"], inplace=True)\n",
    "\n",
    "# Save updated dataset\n",
    "df_merged.to_csv(\"bhel_stock_sentiment_data.csv\", index=False)\n",
    "\n",
    "print(\"\\n✅ Merging Completed Successfully!\")\n",
    "# print(\"March 19 news merged with March 18 stock data to predict March 19 prices.\")\n",
    "print(df_merged.tail())  # Show last few rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dfd6ea-1aa1-4db8-a010-d84c81993fd6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dad44ae-ede1-4428-a88a-24e8c325b37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly Detection:-\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"bhel_stock_sentiment_data.csv\")  # Ensure your CSV has columns Date, Open, High, Low, Close\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "# Initialize Isolation Forest for each OHLC price\n",
    "iso_forest_open = IsolationForest(contamination=0.01, random_state=42)\n",
    "iso_forest_high = IsolationForest(contamination=0.01, random_state=42)\n",
    "iso_forest_low = IsolationForest(contamination=0.01, random_state=42)\n",
    "iso_forest_close = IsolationForest(contamination=0.01, random_state=42)\n",
    "\n",
    "# Detect anomalies for each OHLC price\n",
    "df[\"Anomaly_Open\"] = iso_forest_open.fit_predict(df[['Open']])\n",
    "df[\"Anomaly_High\"] = iso_forest_high.fit_predict(df[['High']])\n",
    "df[\"Anomaly_Low\"] = iso_forest_low.fit_predict(df[['Low']])\n",
    "df[\"Anomaly_Close\"] = iso_forest_close.fit_predict(df[['Close']])\n",
    "\n",
    "# Plot anomalies for Open price\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df.index, df['Open'], label='Open Price', color='blue', linewidth=2)\n",
    "anomalies_open = df[df['Anomaly_Open'] == -1]\n",
    "plt.scatter(anomalies_open.index, anomalies_open['Open'], color='red', label='Anomalies - Open', zorder=5)\n",
    "plt.title(\"Open Price with Anomalies\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Open Price\")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot anomalies for High price\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df.index, df['High'], label='High Price', color='green', linewidth=2)\n",
    "anomalies_high = df[df['Anomaly_High'] == -1]\n",
    "plt.scatter(anomalies_high.index, anomalies_high['High'], color='red', label='Anomalies - High', zorder=5)\n",
    "plt.title(\"High Price with Anomalies\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"High Price\")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot anomalies for Low price\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df.index, df['Low'], label='Low Price', color='orange', linewidth=2)\n",
    "anomalies_low = df[df['Anomaly_Low'] == -1]\n",
    "plt.scatter(anomalies_low.index, anomalies_low['Low'], color='red', label='Anomalies - Low', zorder=5)\n",
    "plt.title(\"Low Price with Anomalies\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Low Price\")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot anomalies for Close price\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df.index, df['Close'], label='Close Price', color='blue', linewidth=2)\n",
    "anomalies_close = df[df['Anomaly_Close'] == -1]\n",
    "plt.scatter(anomalies_close.index, anomalies_close['Close'], color='red', label='Anomalies - Close', zorder=5)\n",
    "plt.title(\"Close Price with Anomalies\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Remove anomalies from the dataset (keep only normal data)\n",
    "df_clean = df[\n",
    "    (df[\"Anomaly_Open\"] == 1) &\n",
    "    (df[\"Anomaly_High\"] == 1) &\n",
    "    (df[\"Anomaly_Low\"] == 1) &\n",
    "    (df[\"Anomaly_Close\"] == 1)\n",
    "].drop(columns=[\"Anomaly_Open\", \"Anomaly_High\", \"Anomaly_Low\", \"Anomaly_Close\"])\n",
    "\n",
    "\n",
    "# Normalize only numeric columns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Select only numeric columns (exclude strings like news/headlines)\n",
    "numeric_cols = df_clean.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(df_clean[numeric_cols]),\n",
    "    columns=numeric_cols,\n",
    "    index=df_clean.index\n",
    ")\n",
    "\n",
    "\n",
    "# Prepare train/test split\n",
    "train_size = int(len(df_scaled) * 0.8)\n",
    "train, test = df_scaled[:train_size], df_scaled[train_size:]\n",
    "\n",
    "# Save the preprocessed data\n",
    "train.to_csv(\"bhel_train_data.csv\")\n",
    "test.to_csv(\"bhel_test_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4612b39-ce0b-4843-8856-1ac355e46344",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e95d3e-1e5b-4d23-9f64-2918b12c7079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model:-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# Set additional environment variables to control TF behavior\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "\n",
    "# Load the preprocessed dataset (after anomaly removal)\n",
    "train = pd.read_csv(\"bhel_train_data.csv\", index_col=\"date\", parse_dates=True)\n",
    "test = pd.read_csv(\"bhel_test_data.csv\", index_col=\"date\", parse_dates=True)\n",
    "\n",
    "# Include sentiment score in the features\n",
    "train = train[['Open', 'High', 'Low', 'Close', 'sentiment_score']]\n",
    "test = test[['Open', 'High', 'Low', 'Close', 'sentiment_score']]\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(train)\n",
    "test_scaled = scaler.transform(test)\n",
    "\n",
    "# Prepare the data for LSTM\n",
    "def create_dataset(data, look_back=60):\n",
    "    X, y = [], []\n",
    "    for i in range(look_back, len(data)):\n",
    "        X.append(data[i-look_back:i])\n",
    "        y.append(data[i, :4])  # Only predict OHLC, not sentiment\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Prepare train and test data\n",
    "X_train, y_train = create_dataset(train_scaled)\n",
    "X_test, y_test = create_dataset(test_scaled)\n",
    "\n",
    "# Reshape for LSTM input\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2])\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=50, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=4))  # Predict Open, High, Low, Close\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, \n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=[early_stop])\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Inverse transform with dummy sentiment_score\n",
    "def add_dummy_sentiment(data):\n",
    "    dummy = np.zeros((data.shape[0], 1))\n",
    "    return np.concatenate((data, dummy), axis=1)\n",
    "\n",
    "y_pred_padded = add_dummy_sentiment(y_pred)\n",
    "y_test_padded = add_dummy_sentiment(y_test)\n",
    "\n",
    "y_pred_inverse = scaler.inverse_transform(y_pred_padded)[:, :4]\n",
    "y_test_inverse = scaler.inverse_transform(y_test_padded)[:, :4]\n",
    "\n",
    "# Evaluation\n",
    "mse = mean_squared_error(y_test_inverse, y_pred_inverse)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test_inverse, y_pred_inverse)\n",
    "r2 = r2_score(y_test_inverse, y_pred_inverse)\n",
    "\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"Accuracy (R²): {r2 * 100}%\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training vs testing data for OHLC prices\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot for Open price\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(train.index, train['Open'], label=\"Train Open\", color='blue')\n",
    "plt.plot(test.index, test['Open'], label=\"Test Open\", color='orange')\n",
    "plt.title(\"Training vs Testing Open Price\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Open Price\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot for High price\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(train.index, train['High'], label=\"Train High\", color='blue')\n",
    "plt.plot(test.index, test['High'], label=\"Test High\", color='orange')\n",
    "plt.title(\"Training vs Testing High Price\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"High Price\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot for Low price\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(train.index, train['Low'], label=\"Train Low\", color='blue')\n",
    "plt.plot(test.index, test['Low'], label=\"Test Low\", color='orange')\n",
    "plt.title(\"Training vs Testing Low Price\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Low Price\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot for Close price\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(train.index, train['Close'], label=\"Train Close\", color='blue')\n",
    "plt.plot(test.index, test['Close'], label=\"Test Close\", color='orange')\n",
    "plt.title(\"Training vs Testing Close Price\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# CNN Model:-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# Deterministic TF behavior\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "# Load the preprocessed dataset (after anomaly removal)\n",
    "train = pd.read_csv(\"bhel_train_data.csv\", index_col=\"date\", parse_dates=True)\n",
    "test = pd.read_csv(\"bhel_test_data.csv\", index_col=\"date\", parse_dates=True)\n",
    "\n",
    "# Include sentiment score in the features\n",
    "train = train[['Open', 'High', 'Low', 'Close', 'sentiment_score']]\n",
    "test = test[['Open', 'High', 'Low', 'Close', 'sentiment_score']]\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(train)\n",
    "test_scaled = scaler.transform(test)\n",
    "\n",
    "# Prepare the data for CNN\n",
    "def create_dataset(data, look_back=60):\n",
    "    X, y = [], []\n",
    "    for i in range(look_back, len(data)):\n",
    "        X.append(data[i-look_back:i])\n",
    "        y.append(data[i, :4])  # Predict only OHLC\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create train and test datasets\n",
    "X_train, y_train = create_dataset(train_scaled)\n",
    "X_test, y_test = create_dataset(test_scaled)\n",
    "\n",
    "# CNN expects 3D input just like LSTM: (samples, timesteps, features)\n",
    "# Already in the correct shape from create_dataset\n",
    "\n",
    "# Build the CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(4))  # Predict OHLC\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=[early_stop])\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Inverse transform with dummy sentiment\n",
    "def add_dummy_sentiment(data):\n",
    "    dummy = np.zeros((data.shape[0], 1))\n",
    "    return np.concatenate((data, dummy), axis=1)\n",
    "\n",
    "y_pred_padded = add_dummy_sentiment(y_pred)\n",
    "y_test_padded = add_dummy_sentiment(y_test)\n",
    "\n",
    "y_pred_inverse = scaler.inverse_transform(y_pred_padded)[:, :4]\n",
    "y_test_inverse = scaler.inverse_transform(y_test_padded)[:, :4]\n",
    "\n",
    "# Evaluation\n",
    "mse = mean_squared_error(y_test_inverse, y_pred_inverse)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test_inverse, y_pred_inverse)\n",
    "r2 = r2_score(y_test_inverse, y_pred_inverse)\n",
    "\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"Accuracy (R²): {r2 * 100}%\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training vs testing data for OHLC prices\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Open\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(train.index, train['Open'], label=\"Train Open\", color='blue')\n",
    "plt.plot(test.index, test['Open'], label=\"Test Open\", color='orange')\n",
    "plt.title(\"Training vs Testing Open Price\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Open Price\")\n",
    "plt.legend()\n",
    "\n",
    "# High\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(train.index, train['High'], label=\"Train High\", color='blue')\n",
    "plt.plot(test.index, test['High'], label=\"Test High\", color='orange')\n",
    "plt.title(\"Training vs Testing High Price\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"High Price\")\n",
    "plt.legend()\n",
    "\n",
    "# Low\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(train.index, train['Low'], label=\"Train Low\", color='blue')\n",
    "plt.plot(test.index, test['Low'], label=\"Test Low\", color='orange')\n",
    "plt.title(\"Training vs Testing Low Price\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Low Price\")\n",
    "plt.legend()\n",
    "\n",
    "# Close\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(train.index, train['Close'], label=\"Train Close\", color='blue')\n",
    "plt.plot(test.index, test['Close'], label=\"Test Close\", color='orange')\n",
    "plt.title(\"Training vs Testing Close Price\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# RNN Model:-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "# Reproducibility\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "# Load dataset\n",
    "train = pd.read_csv(\"bhel_train_data.csv\", index_col=\"date\", parse_dates=True)\n",
    "test = pd.read_csv(\"bhel_test_data.csv\", index_col=\"date\", parse_dates=True)\n",
    "\n",
    "# Select features (OHLC + sentiment)\n",
    "train = train[['Open', 'High', 'Low', 'Close', 'sentiment_score']]\n",
    "test = test[['Open', 'High', 'Low', 'Close', 'sentiment_score']]\n",
    "\n",
    "# Normalize\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(train)\n",
    "test_scaled = scaler.transform(test)\n",
    "\n",
    "# Create sequences\n",
    "def create_dataset(data, look_back=60):\n",
    "    X, y = [], []\n",
    "    for i in range(look_back, len(data)):\n",
    "        X.append(data[i-look_back:i])\n",
    "        y.append(data[i, :4])  # Predict only OHLC\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = create_dataset(train_scaled)\n",
    "X_test, y_test = create_dataset(test_scaled)\n",
    "\n",
    "# Build RNN model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(SimpleRNN(units=50, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=4))  # Output OHLC\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=[early_stop])\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Inverse scale with dummy sentiment\n",
    "def add_dummy_sentiment(data):\n",
    "    dummy = np.zeros((data.shape[0], 1))\n",
    "    return np.concatenate((data, dummy), axis=1)\n",
    "\n",
    "y_pred_padded = add_dummy_sentiment(y_pred)\n",
    "y_test_padded = add_dummy_sentiment(y_test)\n",
    "\n",
    "y_pred_inverse = scaler.inverse_transform(y_pred_padded)[:, :4]\n",
    "y_test_inverse = scaler.inverse_transform(y_test_padded)[:, :4]\n",
    "\n",
    "# Metrics\n",
    "mse = mean_squared_error(y_test_inverse, y_pred_inverse)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test_inverse, y_pred_inverse)\n",
    "r2 = r2_score(y_test_inverse, y_pred_inverse)\n",
    "\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"R² Score: {r2 * 100}%\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "\n",
    "# Plot loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot OHLC training vs testing\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Open\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(train.index, train['Open'], label=\"Train Open\", color='blue')\n",
    "plt.plot(test.index, test['Open'], label=\"Test Open\", color='orange')\n",
    "plt.title(\"Training vs Testing Open Price\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Open Price\")\n",
    "plt.legend()\n",
    "\n",
    "# High\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(train.index, train['High'], label=\"Train High\", color='blue')\n",
    "plt.plot(test.index, test['High'], label=\"Test High\", color='orange')\n",
    "plt.title(\"Training vs Testing High Price\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"High Price\")\n",
    "plt.legend()\n",
    "\n",
    "# Low\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(train.index, train['Low'], label=\"Train Low\", color='blue')\n",
    "plt.plot(test.index, test['Low'], label=\"Test Low\", color='orange')\n",
    "plt.title(\"Training vs Testing Low Price\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Low Price\")\n",
    "plt.legend()\n",
    "\n",
    "# Close\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(train.index, train['Close'], label=\"Train Close\", color='blue')\n",
    "plt.plot(test.index, test['Close'], label=\"Test Close\", color='orange')\n",
    "plt.title(\"Training vs Testing Close Price\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# GRU Model:-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# Set additional environment variables to control TF behavior\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "# Load the preprocessed dataset (after anomaly removal)\n",
    "train = pd.read_csv(\"bhel_train_data.csv\", index_col=\"date\", parse_dates=True)\n",
    "test = pd.read_csv(\"bhel_test_data.csv\", index_col=\"date\", parse_dates=True)\n",
    "\n",
    "# Include sentiment score in the features\n",
    "train = train[['Open', 'High', 'Low', 'Close', 'sentiment_score']]\n",
    "test = test[['Open', 'High', 'Low', 'Close', 'sentiment_score']]\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(train)\n",
    "test_scaled = scaler.transform(test)\n",
    "\n",
    "# Prepare the data for GRU\n",
    "def create_dataset(data, look_back=60):\n",
    "    X, y = [], []\n",
    "    for i in range(look_back, len(data)):\n",
    "        X.append(data[i-look_back:i])\n",
    "        y.append(data[i, :4])  # Only predict OHLC, not sentiment\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = create_dataset(train_scaled)\n",
    "X_test, y_test = create_dataset(test_scaled)\n",
    "\n",
    "# Reshape for GRU input\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2])\n",
    "\n",
    "# Build the GRU model\n",
    "model = Sequential()\n",
    "model.add(GRU(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GRU(units=50, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=4))  # Predict Open, High, Low, Close\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=[early_stop])\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Inverse transform with dummy sentiment\n",
    "def add_dummy_sentiment(data):\n",
    "    dummy = np.zeros((data.shape[0], 1))\n",
    "    return np.concatenate((data, dummy), axis=1)\n",
    "\n",
    "y_pred_padded = add_dummy_sentiment(y_pred)\n",
    "y_test_padded = add_dummy_sentiment(y_test)\n",
    "\n",
    "y_pred_inverse = scaler.inverse_transform(y_pred_padded)[:, :4]\n",
    "y_test_inverse = scaler.inverse_transform(y_test_padded)[:, :4]\n",
    "\n",
    "# Evaluation\n",
    "mse = mean_squared_error(y_test_inverse, y_pred_inverse)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test_inverse, y_pred_inverse)\n",
    "r2 = r2_score(y_test_inverse, y_pred_inverse)\n",
    "\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"Accuracy (R²): {r2 * 100}%\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training vs testing data for OHLC prices\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot for Open price\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(train.index, train['Open'], label=\"Train Open\", color='blue')\n",
    "plt.plot(test.index, test['Open'], label=\"Test Open\", color='orange')\n",
    "plt.title(\"Training vs Testing Open Price\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Open Price\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot for High price\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(train.index, train['High'], label=\"Train High\", color='blue')\n",
    "plt.plot(test.index, test['High'], label=\"Test High\", color='orange')\n",
    "plt.title(\"Training vs Testing High Price\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"High Price\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot for Low price\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(train.index, train['Low'], label=\"Train Low\", color='blue')\n",
    "plt.plot(test.index, test['Low'], label=\"Test Low\", color='orange')\n",
    "plt.title(\"Training vs Testing Low Price\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Low Price\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot for Close price\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(train.index, train['Close'], label=\"Train Close\", color='blue')\n",
    "plt.plot(test.index, test['Close'], label=\"Test Close\", color='orange')\n",
    "plt.title(\"Training vs Testing Close Price\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# TCN Model:-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tcn import TCN\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "0.004\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "# Load the preprocessed dataset (after anomaly removal)\n",
    "train = pd.read_csv(\"bhel_train_data.csv\", index_col=\"date\", parse_dates=True)\n",
    "test = pd.read_csv(\"bhel_test_data.csv\", index_col=\"date\", parse_dates=True)\n",
    "\n",
    "# Include sentiment score in the features\n",
    "train = train[['Open', 'High', 'Low', 'Close', 'sentiment_score']]\n",
    "test = test[['Open', 'High', 'Low', 'Close', 'sentiment_score']]\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(train)\n",
    "test_scaled = scaler.transform(test)\n",
    "\n",
    "# Prepare data\n",
    "def create_dataset(data, look_back=60):\n",
    "    X, y = [], []\n",
    "    for i in range(look_back, len(data)):\n",
    "        X.append(data[i - look_back:i])\n",
    "        y.append(data[i, :4])  # Predict OHLC only\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = create_dataset(train_scaled)\n",
    "X_test, y_test = create_dataset(test_scaled)\n",
    "\n",
    "# Build the TCN model\n",
    "model = Sequential()\n",
    "model.add(TCN(input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=4))  # Predict Open, High, Low, Close\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, \n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=[early_stop])\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Inverse transform\n",
    "def add_dummy_sentiment(data):\n",
    "    dummy = np.zeros((data.shape[0], 1))\n",
    "    return np.concatenate((data, dummy), axis=1)\n",
    "\n",
    "y_pred_padded = add_dummy_sentiment(y_pred)\n",
    "y_test_padded = add_dummy_sentiment(y_test)\n",
    "\n",
    "y_pred_inverse = scaler.inverse_transform(y_pred_padded)[:, :4]\n",
    "y_test_inverse = scaler.inverse_transform(y_test_padded)[:, :4]\n",
    "\n",
    "# Evaluation\n",
    "mse = mean_squared_error(y_test_inverse, y_pred_inverse)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test_inverse, y_pred_inverse)\n",
    "r2 = r2_score(y_test_inverse, y_pred_inverse)\n",
    "\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"Accuracy (R²): {r2 * 100}%\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training vs testing data for OHLC prices\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(train.index, train['Open'], label=\"Train Open\", color='blue')\n",
    "plt.plot(test.index, test['Open'], label=\"Test Open\", color='orange')\n",
    "plt.title(\"Training vs Testing Open Price\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Open Price\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(train.index, train['High'], label=\"Train High\", color='blue')\n",
    "plt.plot(test.index, test['High'], label=\"Test High\", color='orange')\n",
    "plt.title(\"Training vs Testing High Price\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"High Price\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(train.index, train['Low'], label=\"Train Low\", color='blue')\n",
    "plt.plot(test.index, test['Low'], label=\"Test Low\", color='orange')\n",
    "plt.title(\"Training vs Testing Low Price\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Low Price\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(train.index, train['Close'], label=\"Train Close\", color='blue')\n",
    "plt.plot(test.index, test['Close'], label=\"Test Close\", color='orange')\n",
    "plt.title(\"Training vs Testing Close Price\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# XGBoost Model:-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "# Load the preprocessed dataset (after anomaly removal)\n",
    "train = pd.read_csv(\"bhel_train_data.csv\", index_col=\"date\", parse_dates=True)\n",
    "test = pd.read_csv(\"bhel_test_data.csv\", index_col=\"date\", parse_dates=True)\n",
    "\n",
    "# Include sentiment score in the features\n",
    "train = train[['Open', 'High', 'Low', 'Close', 'sentiment_score']]\n",
    "test = test[['Open', 'High', 'Low', 'Close', 'sentiment_score']]\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(train)\n",
    "test_scaled = scaler.transform(test)\n",
    "\n",
    "# Prepare the data for XGBoost\n",
    "def create_dataset(data, look_back=60):\n",
    "    X, y = [], []\n",
    "    for i in range(look_back, len(data)):\n",
    "        X.append(data[i-look_back:i].flatten())  # Flatten for XGBoost\n",
    "        y.append(data[i, :4])  # Only predict OHLC\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Prepare train and test data\n",
    "X_train, y_train = create_dataset(train_scaled)\n",
    "X_test, y_test = create_dataset(test_scaled)\n",
    "\n",
    "# Train separate XGBoost models for each OHLC value\n",
    "models = []\n",
    "predictions = []\n",
    "for i in range(4):\n",
    "    model = XGBRegressor(n_estimators=100, learning_rate=0.05, max_depth=5, random_state=seed)\n",
    "    model.fit(X_train, y_train[:, i])\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions.append(y_pred)\n",
    "    models.append(model)\n",
    "\n",
    "# Combine predictions\n",
    "y_pred = np.stack(predictions, axis=1)\n",
    "\n",
    "# Inverse transform with dummy sentiment_score\n",
    "def add_dummy_sentiment(data):\n",
    "    dummy = np.zeros((data.shape[0], 1))\n",
    "    return np.concatenate((data, dummy), axis=1)\n",
    "\n",
    "y_pred_padded = add_dummy_sentiment(y_pred)\n",
    "y_test_padded = add_dummy_sentiment(y_test)\n",
    "\n",
    "y_pred_inverse = scaler.inverse_transform(y_pred_padded)[:, :4]\n",
    "y_test_inverse = scaler.inverse_transform(y_test_padded)[:, :4]\n",
    "\n",
    "# Evaluation\n",
    "mse = mean_squared_error(y_test_inverse, y_pred_inverse)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test_inverse, y_pred_inverse)\n",
    "r2 = r2_score(y_test_inverse, y_pred_inverse)\n",
    "\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"Accuracy (R²): {r2 * 100}%\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "\n",
    "\n",
    "# Plot training vs testing data for OHLC prices\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot for Open price\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(train.index, train['Open'], label=\"Train Open\", color='blue')\n",
    "plt.plot(test.index, test['Open'], label=\"Test Open\", color='orange')\n",
    "plt.title(\"Training vs Testing Open Price\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Open Price\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot for High price\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(train.index, train['High'], label=\"Train High\", color='blue')\n",
    "plt.plot(test.index, test['High'], label=\"Test High\", color='orange')\n",
    "plt.title(\"Training vs Testing High Price\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"High Price\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot for Low price\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(train.index, train['Low'], label=\"Train Low\", color='blue')\n",
    "plt.plot(test.index, test['Low'], label=\"Test Low\", color='orange')\n",
    "plt.title(\"Training vs Testing Low Price\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Low Price\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot for Close price\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(train.index, train['Close'], label=\"Train Close\", color='blue')\n",
    "plt.plot(test.index, test['Close'], label=\"Test Close\", color='orange')\n",
    "plt.title(\"Training vs Testing Close Price\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# LightGBM Model:-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Load the preprocessed dataset (after anomaly removal)\n",
    "train = pd.read_csv(\"bhel_train_data.csv\", index_col=\"date\", parse_dates=True)\n",
    "test = pd.read_csv(\"bhel_test_data.csv\", index_col=\"date\", parse_dates=True)\n",
    "\n",
    "# Include sentiment score in the features\n",
    "train = train[['Open', 'High', 'Low', 'Close', 'sentiment_score']]\n",
    "test = test[['Open', 'High', 'Low', 'Close', 'sentiment_score']]\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(train)\n",
    "test_scaled = scaler.transform(test)\n",
    "\n",
    "# Create sequences (look_back=60)\n",
    "def create_dataset(data, look_back=60):\n",
    "    X, y = [], []\n",
    "    for i in range(look_back, len(data)):\n",
    "        features = data[i-look_back:i].reshape(-1)\n",
    "        X.append(features)\n",
    "        y.append(data[i, :4])  # Only predict OHLC, not sentiment\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = create_dataset(train_scaled)\n",
    "X_test, y_test = create_dataset(test_scaled)\n",
    "\n",
    "# Build and train separate LightGBM models for each OHLC value\n",
    "y_pred = np.zeros_like(y_test)\n",
    "\n",
    "for i, label in enumerate(['Open', 'High', 'Low', 'Close']):\n",
    "    model = lgb.LGBMRegressor(n_estimators=30, random_state=seed)\n",
    "    model.fit(X_train, y_train[:, i])\n",
    "    y_pred[:, i] = model.predict(X_test)\n",
    "\n",
    "# Inverse transform with dummy sentiment\n",
    "def add_dummy_sentiment(data):\n",
    "    dummy = np.zeros((data.shape[0], 1))\n",
    "    return np.concatenate((data, dummy), axis=1)\n",
    "\n",
    "y_pred_padded = add_dummy_sentiment(y_pred)\n",
    "y_test_padded = add_dummy_sentiment(y_test)\n",
    "\n",
    "y_pred_inverse = scaler.inverse_transform(y_pred_padded)[:, :4]\n",
    "y_test_inverse = scaler.inverse_transform(y_test_padded)[:, :4]\n",
    "\n",
    "# Evaluation\n",
    "mse = mean_squared_error(y_test_inverse, y_pred_inverse)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test_inverse, y_pred_inverse)\n",
    "r2 = r2_score(y_test_inverse, y_pred_inverse)\n",
    "\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"Accuracy (R²): {r2 * 100}%\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "\n",
    "# Plotting Loss is skipped as LightGBM doesn’t return per-epoch history\n",
    "# You can enable this with `model.booster_.evals_result_()` if needed\n",
    "\n",
    "# Plot training vs testing data for OHLC prices\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot for Open price\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(train.index, train['Open'], label=\"Train Open\", color='blue')\n",
    "plt.plot(test.index, test['Open'], label=\"Test Open\", color='orange')\n",
    "plt.title(\"Training vs Testing Open Price\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Open Price\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot for High price\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(train.index, train['High'], label=\"Train High\", color='blue')\n",
    "plt.plot(test.index, test['High'], label=\"Test High\", color='orange')\n",
    "plt.title(\"Training vs Testing High Price\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"High Price\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot for Low price\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(train.index, train['Low'], label=\"Train Low\", color='blue')\n",
    "plt.plot(test.index, test['Low'], label=\"Test Low\", color='orange')\n",
    "plt.title(\"Training vs Testing Low Price\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Low Price\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot for Close price\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(train.index, train['Close'], label=\"Train Close\", color='blue')\n",
    "plt.plot(test.index, test['Close'], label=\"Test Close\", color='orange')\n",
    "plt.title(\"Training vs Testing Close Price\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
